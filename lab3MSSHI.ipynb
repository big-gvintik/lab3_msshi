{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94168c5d",
   "metadata": {},
   "source": [
    "\n",
    "# Tomato Detection Pipeline (YOLOv8, Faster R-CNN, RetinaNet, FCOS)\n",
    "\n",
    "This notebook walks through the full training and evaluation pipeline for tomato object detection. It covers dataset preparation, training three TorchVision detectors (Faster R-CNN, RetinaNet, FCOS), training YOLOv8, and computing COCO metrics and qualitative visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1bf96c",
   "metadata": {},
   "source": [
    "\n",
    "## Notebook workflow\n",
    "\n",
    "1. Configure hyperparameters that work well for this dataset and task.\n",
    "2. Prepare the dataset in both YOLO and COCO formats.\n",
    "3. Train YOLOv8 and evaluate on the validation split.\n",
    "4. Train TorchVision detectors (Faster R-CNN, RetinaNet, FCOS) with matched settings.\n",
    "5. Compare metrics and export qualitative predictions.\n",
    "\n",
    "> **Tip:** Update the configuration or runtime parameters below before running the execution cell at the end of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f875f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kaggle import api as kaggle_api\n",
    "from ultralytics import YOLO\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5269c1c2",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration\n",
    "\n",
    "The hyperparameters below are tuned for strong detection accuracy on the tomato dataset:\n",
    "\n",
    "- **YOLOv8:** 50 epochs with batch size 16 and 640px training resolution balance accuracy and runtime.\n",
    "- **TorchVision detectors:** 20 epochs with AdamW (lr `2.5e-4`, weight decay `1e-4`) work well for Faster R-CNN, RetinaNet, and FCOS.\n",
    "- **FCOS:** Uses pretrained weights for both backbone and detection head while consuming 1-based class labels (matching Faster R-CNN).\n",
    "\n",
    "Adjust any values to experiment with different trade-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b995466",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    # Dataset\n",
    "    \"dataset_dir\": \"\",  # if empty, set DATASET_DIR below or enable Kaggle download\n",
    "    \"work_dir\": \"work_tomato\",\n",
    "    \"outputs_dir\": \"outputs\",\n",
    "    \"classes\": [\"tomato\"],\n",
    "\n",
    "    # YOLOv8\n",
    "    \"yolo\": {\n",
    "        \"weights\": \"yolov8n.pt\",\n",
    "        \"epochs\": 50,\n",
    "        \"imgsz\": 640,\n",
    "        \"batch\": 16,\n",
    "        \"eval_conf\": 0.001\n",
    "    },\n",
    "\n",
    "    # TorchVision shared hyperparameters\n",
    "    \"tv\": {\n",
    "        \"epochs\": 20,\n",
    "        \"batch_train\": 4,\n",
    "        \"batch_val\": 2,\n",
    "        \"lr\": 2.5e-4,\n",
    "        \"weight_decay\": 1e-4\n",
    "    },\n",
    "\n",
    "    # Faster R-CNN\n",
    "    \"frcnn\": {\n",
    "        \"weights\": \"DEFAULT\",\n",
    "        \"label_offset\": 0,\n",
    "        \"cat_id_offset\": 0\n",
    "    },\n",
    "\n",
    "    # RetinaNet\n",
    "    \"retinanet\": {\n",
    "        \"weights\": None,\n",
    "        \"weights_backbone\": \"DEFAULT\",\n",
    "        \"num_classes\": 1,\n",
    "        \"label_offset\": -1,\n",
    "        \"cat_id_offset\": 1,\n",
    "        \"eval_conf\": 0.0\n",
    "    },\n",
    "\n",
    "    # FCOS\n",
    "    \"fcos\": {\n",
    "        \"weights\": \"DEFAULT\",\n",
    "        \"weights_backbone\": \"DEFAULT\",\n",
    "        \"label_offset\": 0,\n",
    "        \"cat_id_offset\": 0,\n",
    "        \"eval_conf\": 0.0\n",
    "    },\n",
    "\n",
    "    # COCO evaluation\n",
    "    \"coco\": {\n",
    "        \"maxDets\": [1, 10, 100]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7c448",
   "metadata": {},
   "source": [
    "\n",
    "### Runtime overrides\n",
    "\n",
    "Specify filesystem paths or override epoch counts before launching the main execution cell.\n",
    "\n",
    "- `DATASET_DIR`: Path to a folder containing `images/` and annotation files (Pascal VOC XML or YOLO TXT). Leave as `None` to rely on Kaggle download.\n",
    "- `SKIP_KAGGLE`: Set to `True` to avoid downloading (expects dataset to exist locally under `./datasets`).\n",
    "- `EPOCHS_YOLO` / `EPOCHS_TV`: Quickly change the number of epochs without editing `CONFIG`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c892fecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATASET_DIR = None  # e.g. Path(\"/path/to/tomato_dataset\")\n",
    "SKIP_KAGGLE = False\n",
    "EPOCHS_YOLO = CONFIG[\"yolo\"][\"epochs\"]\n",
    "EPOCHS_TV = CONFIG[\"tv\"][\"epochs\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abdb4b",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset preparation helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ffebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\", \".JPG\", \".PNG\"}\n",
    "\n",
    "\n",
    "def device_info():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    name = torch.cuda.get_device_name(0) if use_cuda else \"CPU\"\n",
    "    print(f\"CUDA available: {use_cuda} | Device: {name}\")\n",
    "    return \"cuda:0\" if use_cuda else \"cpu\"\n",
    "\n",
    "\n",
    "def resolve_tomato_root(base: Path) -> Path:\n",
    "    base = base.resolve()\n",
    "\n",
    "    def has_images(p: Path) -> bool:\n",
    "        try:\n",
    "            return any(f.suffix.lower() in IMG_EXTS for f in p.rglob(\"*\"))\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    if base.exists() and base.is_dir() and has_images(base):\n",
    "        return base\n",
    "    if base.name.lower() == \"annotations\" and base.parent.exists():\n",
    "        par = base.parent\n",
    "        if has_images(par):\n",
    "            return par\n",
    "    if base.parent.exists():\n",
    "        for cand in [base] + [p for p in base.parent.iterdir() if p.is_dir()]:\n",
    "            img_dir = cand / \"images\"\n",
    "            if img_dir.exists() and img_dir.is_dir() and has_images(img_dir):\n",
    "                return cand\n",
    "    if base.parent.exists() and has_images(base.parent):\n",
    "        return base.parent\n",
    "    return base\n",
    "\n",
    "\n",
    "def download_variant10_dataset(root: Path) -> Path:\n",
    "    ds_root = root / \"datasets\"\n",
    "    ds_root.mkdir(parents=True, exist_ok=True)\n",
    "    slug = \"andrewmvd/tomato-detection\"\n",
    "    print(f\"Downloading Kaggle dataset: {slug} -> {ds_root}\")\n",
    "    kaggle_api.dataset_download_files(dataset=slug, path=str(ds_root), unzip=True)\n",
    "    return resolve_tomato_root(ds_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c3e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def voc_parse_xml(xml_path: Path):\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    tree = ET.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "    size = root.find(\"size\")\n",
    "    w = int(size.find(\"width\").text)\n",
    "    h = int(size.find(\"height\").text)\n",
    "    objs = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        name = obj.find(\"name\").text.strip().lower()\n",
    "        bnd = obj.find(\"bndbox\")\n",
    "        xmin = int(float(bnd.find(\"xmin\").text))\n",
    "        ymin = int(float(bnd.find(\"ymin\").text))\n",
    "        xmax = int(float(bnd.find(\"xmax\").text))\n",
    "        ymax = int(float(bnd.find(\"ymax\").text))\n",
    "        objs.append((name, xmin, ymin, xmax, ymax, w, h))\n",
    "    return objs, w, h\n",
    "\n",
    "\n",
    "def voc_bbox_to_yolo(xmin, ymin, xmax, ymax, iw, ih):\n",
    "    x_c = (xmin + xmax) / 2.0 / iw\n",
    "    y_c = (ymin + ymax) / 2.0 / ih\n",
    "    bw = (xmax - xmin) / iw\n",
    "    bh = (ymax - ymin) / ih\n",
    "    return x_c, y_c, bw, bh\n",
    "\n",
    "\n",
    "def prepare_yolo_and_coco(raw_root: Path, work_root: Path, classes: List[str]):\n",
    "    if work_root.exists():\n",
    "        shutil.rmtree(work_root)\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        (work_root / \"images\" / sp).mkdir(parents=True, exist_ok=True)\n",
    "        (work_root / \"labels\" / sp).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    images = [p for p in raw_root.rglob(\"*\") if p.suffix in IMG_EXTS]\n",
    "    xmls = {p.stem: p for p in raw_root.rglob(\"*.xml\")}\n",
    "    images = list({p.resolve() for p in images})\n",
    "    random.seed(42)\n",
    "    random.shuffle(images)\n",
    "    n = len(images)\n",
    "    n_train = max(1, int(0.7 * n))\n",
    "    n_val = max(1, int(0.2 * n))\n",
    "    n_test = max(1, n - n_train - n_val)\n",
    "    splits = {\"train\": images[:n_train], \"val\": images[n_train:n_train + n_val], \"test\": images[n_train + n_val:]}\n",
    "    print({k: len(v) for k, v in splits.items()})\n",
    "    if n == 0:\n",
    "        raise RuntimeError(f\"No images found under {raw_root}. Ensure the dataset contains images and annotations.\")\n",
    "\n",
    "    class_to_id = {c: i for i, c in enumerate(classes)}\n",
    "    for sp, paths in splits.items():\n",
    "        for img_path in tqdm(paths, desc=f\"Copy+Label {sp}\"):\n",
    "            dst_img = work_root / \"images\" / sp / img_path.name\n",
    "            shutil.copy(img_path, dst_img)\n",
    "            lbl_path = work_root / \"labels\" / sp / f\"{img_path.stem}.txt\"\n",
    "            lbl_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            lines = []\n",
    "            native = img_path.with_suffix(\".txt\")\n",
    "            if native.exists():\n",
    "                lines = [ln.strip() for ln in native.read_text().splitlines()]\n",
    "            else:\n",
    "                if img_path.stem in xmls:\n",
    "                    objs, iw, ih = voc_parse_xml(xmls[img_path.stem])\n",
    "                    for name, xmin, ymin, xmax, ymax, iw, ih in objs:\n",
    "                        cid = class_to_id.get(name, 0)\n",
    "                        x, y, w, h = voc_bbox_to_yolo(xmin, ymin, xmax, ymax, iw, ih)\n",
    "                        lines.append(f\"{cid} {x:.6f} {y:.6f} {w:.6f} {h:.6f}\")\n",
    "            with open(lbl_path, \"w\") as f:\n",
    "                f.write(\"\n",
    "\".join(lines))\n",
    "\n",
    "    data_yaml = work_root / \"data.yaml\"\n",
    "    data_yaml.write_text(\n",
    "        f\"train: {str((work_root/'images'/'train').resolve())}\n",
    "\"\n",
    "        f\"val: {str((work_root/'images'/'val').resolve())}\n",
    "\"\n",
    "        f\"test: {str((work_root/'images'/'test').resolve())}\n",
    "\n",
    "\"\n",
    "        f\"nc: {len(classes)}\n",
    "\"\n",
    "        f\"names: {classes}\n",
    "\"\n",
    "    )\n",
    "    print(\"Wrote\", data_yaml)\n",
    "\n",
    "    def yolo_to_coco(split: str) -> Dict:\n",
    "        from PIL import Image as PILImage\n",
    "\n",
    "        img_dir = work_root / \"images\" / split\n",
    "        lbl_dir = work_root / \"labels\" / split\n",
    "        imgs = [p for p in img_dir.glob(\"*\") if p.suffix in IMG_EXTS]\n",
    "        imgs.sort()\n",
    "        categories = [{\"id\": i + 1, \"name\": c, \"supercategory\": \"object\"} for i, c in enumerate(classes)]\n",
    "        images_json, annotations = [], []\n",
    "        ann_id = 1\n",
    "        for img_id, p in enumerate(imgs, 1):\n",
    "            w, h = PILImage.open(p).size\n",
    "            images_json.append({\"id\": img_id, \"file_name\": str(p), \"width\": w, \"height\": h})\n",
    "            lp = lbl_dir / f\"{p.stem}.txt\"\n",
    "            if lp.exists():\n",
    "                for ln in lp.read_text().splitlines():\n",
    "                    if not ln.strip():\n",
    "                        continue\n",
    "                    cid_s, x, y, bw, bh = ln.strip().split()\n",
    "                    cid = int(cid_s)\n",
    "                    x = float(x)\n",
    "                    y = float(y)\n",
    "                    bw = float(bw)\n",
    "                    bh = float(bh)\n",
    "                    x0 = (x - bw / 2) * w\n",
    "                    y0 = (y - bh / 2) * h\n",
    "                    aw = bw * w\n",
    "                    ah = bh * h\n",
    "                    annotations.append({\n",
    "                        \"id\": ann_id,\n",
    "                        \"image_id\": img_id,\n",
    "                        \"category_id\": cid + 1,\n",
    "                        \"bbox\": [x0, y0, aw, ah],\n",
    "                        \"area\": aw * ah,\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"segmentation\": []\n",
    "                    })\n",
    "                    ann_id += 1\n",
    "        info = {\n",
    "            \"description\": \"Tomato Detection Variant 10\",\n",
    "            \"url\": \"\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2025,\n",
    "            \"contributor\": \"student\",\n",
    "            \"date_created\": datetime.now().isoformat()\n",
    "        }\n",
    "        licenses = []\n",
    "        return {\"info\": info, \"licenses\": licenses, \"images\": images_json, \"annotations\": annotations, \"categories\": categories}\n",
    "\n",
    "    for sp in [\"train\", \"val\", \"test\"]:\n",
    "        coco = yolo_to_coco(sp)\n",
    "        out_json = work_root / f\"coco_{sp}.json\"\n",
    "        out_json.write_text(json.dumps(coco))\n",
    "        print(\"Wrote\", out_json)\n",
    "\n",
    "    return data_yaml, work_root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36cf37a",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24781f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_yolo_boxes(img_path: Path, lbl_path: Path, names: List[str]):\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    if lbl_path.exists():\n",
    "        for ln in lbl_path.read_text().splitlines():\n",
    "            if not ln.strip():\n",
    "                continue\n",
    "            cid, x, y, bw, bh = ln.split()\n",
    "            cid = int(cid)\n",
    "            x = float(x)\n",
    "            y = float(y)\n",
    "            bw = float(bw)\n",
    "            bh = float(bh)\n",
    "            x0 = int((x - bw / 2) * w)\n",
    "            y0 = int((y - bh / 2) * h)\n",
    "            x1 = int((x + bw / 2) * w)\n",
    "            y1 = int((y + bh / 2) * h)\n",
    "            cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "            cv2.putText(img, names[cid], (x0, max(0, y0 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531764e6",
   "metadata": {},
   "source": [
    "\n",
    "## YOLOv8 training and COCO evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eab650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def yolo_manual_coco_eval(yolo_model, work_root: Path, device: str, conf_thr: float, maxDets: List[int]):\n",
    "    gt_json = work_root / \"coco_val.json\"\n",
    "    coco_gt = COCO(str(gt_json))\n",
    "    img_id_map = {Path(im[\"file_name\"]).name: im[\"id\"] for im in coco_gt.dataset[\"images\"]}\n",
    "    val_dir = work_root / \"images\" / \"val\"\n",
    "    val_imgs = sorted([p for p in val_dir.glob(\"*\") if p.suffix.lower() in IMG_EXTS], key=lambda x: x.name)\n",
    "    if len(val_imgs) == 0:\n",
    "        raise RuntimeError(f\"No val images in {val_dir}\")\n",
    "\n",
    "    res = yolo_model.predict([str(p) for p in val_imgs], imgsz=CONFIG[\"yolo\"][\"imgsz\"], conf=conf_thr, device=0 if device.startswith(\"cuda\") else \"cpu\", verbose=False)\n",
    "    detections = []\n",
    "    for p, r in zip(val_imgs, res):\n",
    "        name = p.name\n",
    "        image_id = img_id_map.get(name, None)\n",
    "        if image_id is None:\n",
    "            stem_map = {Path(im[\"file_name\"]).stem: im[\"id\"] for im in coco_gt.dataset[\"images\"]}\n",
    "            image_id = stem_map.get(p.stem, None)\n",
    "        if image_id is None:\n",
    "            continue\n",
    "        if r.boxes is None or len(r.boxes) == 0:\n",
    "            continue\n",
    "        xyxy = r.boxes.xyxy.cpu().numpy()\n",
    "        confs = r.boxes.conf.cpu().numpy()\n",
    "        clss = r.boxes.cls.cpu().numpy().astype(int)\n",
    "        xywh = xyxy.copy()\n",
    "        xywh[:, 2] = xyxy[:, 2] - xyxy[:, 0]\n",
    "        xywh[:, 3] = xyxy[:, 3] - xyxy[:, 1]\n",
    "        xywh[:, 0] = xyxy[:, 0]\n",
    "        xywh[:, 1] = xyxy[:, 1]\n",
    "        for b, s, c in zip(xywh, confs, clss):\n",
    "            detections.append({\n",
    "                \"image_id\": int(image_id),\n",
    "                \"category_id\": int(c) + 1,\n",
    "                \"bbox\": [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                \"score\": float(s)\n",
    "            })\n",
    "\n",
    "    if not detections:\n",
    "        cat_ids = coco_gt.getCatIds()\n",
    "        per_class = {coco_gt.loadCats([cid])[0][\"name\"]: 0.0 for cid in cat_ids}\n",
    "        return {\"mAP_50_95\": 0.0, \"mAP_50\": 0.0, \"AR100\": 0.0, \"AP_per_class\": per_class}\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(detections)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.maxDets = maxDets\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP_50_95\": float(coco_eval.stats[0]),\n",
    "        \"mAP_50\": float(coco_eval.stats[1]),\n",
    "        \"AR100\": float(coco_eval.stats[8]),\n",
    "        \"AP_per_class\": {}\n",
    "    }\n",
    "    for cid in coco_gt.getCatIds():\n",
    "        ce = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "        ce.params.catIds = [cid]\n",
    "        ce.params.maxDets = maxDets\n",
    "        ce.evaluate()\n",
    "        ce.accumulate()\n",
    "        ce.summarize()\n",
    "        name = coco_gt.loadCats([cid])[0][\"name\"]\n",
    "        metrics[\"AP_per_class\"][name] = float(ce.stats[0])\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_eval_yolo(data_yaml: Path, work_root: Path, device: str):\n",
    "    yolo = YOLO(CONFIG[\"yolo\"][\"weights\"])\n",
    "    yolo.train(\n",
    "        data=str(data_yaml),\n",
    "        epochs=CONFIG[\"yolo\"][\"epochs\"],\n",
    "        imgsz=CONFIG[\"yolo\"][\"imgsz\"],\n",
    "        batch=CONFIG[\"yolo\"][\"batch\"],\n",
    "        project=str((work_root / \"runs_yolo\").resolve()),\n",
    "        name=\"tomato_yolo\",\n",
    "        device=0 if device.startswith(\"cuda\") else \"cpu\",\n",
    "        verbose=True\n",
    "    )\n",
    "    yolo.val(\n",
    "        data=str(data_yaml),\n",
    "        imgsz=CONFIG[\"yolo\"][\"imgsz\"],\n",
    "        save_json=False,\n",
    "        project=str((work_root / \"runs_yolo\").resolve()),\n",
    "        name=\"tomato_yolo_val\",\n",
    "        device=0 if device.startswith(\"cuda\") else \"cpu\",\n",
    "        verbose=True\n",
    "    )\n",
    "    metrics = yolo_manual_coco_eval(yolo, work_root, device, conf_thr=CONFIG[\"yolo\"][\"eval_conf\"], maxDets=CONFIG[\"coco\"][\"maxDets\"])\n",
    "    return yolo, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508a5af",
   "metadata": {},
   "source": [
    "\n",
    "## TorchVision dataset and model builders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class YoloDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_dir: Path, lbl_dir: Path, classes: List[str], transforms=None, label_offset: int = 0):\n",
    "        self.imgs = sorted([p for p in img_dir.glob(\"*\") if p.suffix.lower() in IMG_EXTS])\n",
    "        self.lbl_dir = lbl_dir\n",
    "        self.classes = classes\n",
    "        self.transforms = transforms\n",
    "        self.label_offset = label_offset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.imgs[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        lbl_path = self.lbl_dir / f\"{p.stem}.txt\"\n",
    "        boxes, labels = [], []\n",
    "        if lbl_path.exists():\n",
    "            for ln in lbl_path.read_text().splitlines():\n",
    "                if not ln.strip():\n",
    "                    continue\n",
    "                cid, x, y, bw, bh = ln.split()\n",
    "                cid = int(cid)\n",
    "                cid = cid + 1 + self.label_offset\n",
    "                x = float(x)\n",
    "                y = float(y)\n",
    "                bw = float(bw)\n",
    "                bh = float(bh)\n",
    "                x0 = (x - bw / 2) * w\n",
    "                y0 = (y - bh / 2) * h\n",
    "                x1 = (x + bw / 2) * w\n",
    "                y1 = (y + bh / 2) * h\n",
    "                boxes.append([x0, y0, x1, y1])\n",
    "                labels.append(cid)\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, target, str(p)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, targets, paths = zip(*batch)\n",
    "    return list(imgs), list(targets), list(paths)\n",
    "\n",
    "\n",
    "def build_fasterrcnn(num_fg_classes: int):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=CONFIG[\"frcnn\"][\"weights\"])\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_fg_classes + 1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_retinanet(num_fg_classes: int):\n",
    "    model = torchvision.models.detection.retinanet_resnet50_fpn(\n",
    "        weights=CONFIG[\"retinanet\"][\"weights\"],\n",
    "        weights_backbone=CONFIG[\"retinanet\"][\"weights_backbone\"],\n",
    "        num_classes=num_fg_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_fcos(num_fg_classes: int):\n",
    "    model = torchvision.models.detection.fcos_resnet50_fpn(\n",
    "        weights=CONFIG[\"fcos\"][\"weights\"],\n",
    "        weights_backbone=CONFIG[\"fcos\"][\"weights_backbone\"],\n",
    "        num_classes=num_fg_classes\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87c2e5d",
   "metadata": {},
   "source": [
    "\n",
    "## COCO evaluation helpers for TorchVision models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26babb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_to_coco_json(model, dl, device: str, coco_gt_json: Path, score_thresh: float, category_id_offset: int, maxDets: List[int]) -> Tuple[Dict, List[Dict]]:\n",
    "    model.eval()\n",
    "    coco_gt = COCO(str(coco_gt_json))\n",
    "    results = []\n",
    "    img_id_map = {Path(im[\"file_name\"]).name: im[\"id\"] for im in coco_gt.dataset[\"images\"]}\n",
    "    for imgs, targets, paths in tqdm(dl, desc=\"Infer val\"):\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        outputs = model(imgs)\n",
    "        for out, p in zip(outputs, paths):\n",
    "            boxes = out[\"boxes\"].detach().cpu().numpy()\n",
    "            scores = out[\"scores\"].detach().cpu().numpy()\n",
    "            labels = out[\"labels\"].detach().cpu().numpy()\n",
    "            if boxes.shape[0] > 0:\n",
    "                xywh = box_convert(torch.from_numpy(boxes), in_fmt=\"xyxy\", out_fmt=\"xywh\").numpy()\n",
    "            else:\n",
    "                xywh = np.zeros((0, 4), dtype=np.float32)\n",
    "            img_name = Path(p).name\n",
    "            image_id = img_id_map.get(img_name)\n",
    "            if image_id is None:\n",
    "                stem_map = {Path(im[\"file_name\"]).stem: im[\"id\"] for im in coco_gt.dataset[\"images\"]}\n",
    "                image_id = stem_map.get(Path(p).stem, None)\n",
    "            if image_id is None:\n",
    "                continue\n",
    "            for b, s, lb in zip(xywh, scores, labels):\n",
    "                if s < score_thresh:\n",
    "                    continue\n",
    "                results.append({\n",
    "                    \"image_id\": int(image_id),\n",
    "                    \"category_id\": int(lb) + category_id_offset,\n",
    "                    \"bbox\": [float(b[0]), float(b[1]), float(b[2]), float(b[3])],\n",
    "                    \"score\": float(s)\n",
    "                })\n",
    "    return coco_gt, results\n",
    "\n",
    "\n",
    "def coco_eval_from_results(coco_gt: COCO, detections: List[Dict], maxDets: List[int]):\n",
    "    if not detections:\n",
    "        cat_ids = coco_gt.getCatIds()\n",
    "        per_class = {coco_gt.loadCats([cid])[0][\"name\"]: 0.0 for cid in cat_ids}\n",
    "        return {\"mAP_50_95\": 0.0, \"mAP_50\": 0.0, \"AR100\": 0.0, \"AP_per_class\": per_class}\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(detections)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.params.maxDets = maxDets\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    metrics = {\n",
    "        \"mAP_50_95\": float(coco_eval.stats[0]),\n",
    "        \"mAP_50\": float(coco_eval.stats[1]),\n",
    "        \"AR100\": float(coco_eval.stats[8])\n",
    "    }\n",
    "    per_class = {}\n",
    "    for cid in coco_gt.getCatIds():\n",
    "        ce = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "        ce.params.catIds = [cid]\n",
    "        ce.params.maxDets = maxDets\n",
    "        ce.evaluate()\n",
    "        ce.accumulate()\n",
    "        ce.summarize()\n",
    "        name = coco_gt.loadCats([cid])[0][\"name\"]\n",
    "        per_class[name] = float(ce.stats[0])\n",
    "    metrics[\"AP_per_class\"] = per_class\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71dcf27",
   "metadata": {},
   "source": [
    "\n",
    "## Training loop for TorchVision detectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_torchvision_detector(model_type: str, work_root: Path, device: str, classes: List[str], epochs: int, lr: float, wd: float):\n",
    "    img_tr = work_root / \"images\" / \"train\"\n",
    "    lbl_tr = work_root / \"labels\" / \"train\"\n",
    "    img_val = work_root / \"images\" / \"val\"\n",
    "    lbl_val = work_root / \"labels\" / \"val\"\n",
    "    tform = transforms.Compose([transforms.ToTensor()])\n",
    "    num_fg = len(classes)\n",
    "\n",
    "    if model_type == \"fasterrcnn\":\n",
    "        model = build_fasterrcnn(num_fg)\n",
    "        ds_tr = YoloDetectionDataset(img_tr, lbl_tr, classes, transforms=tform, label_offset=CONFIG[\"frcnn\"][\"label_offset\"])\n",
    "        ds_val = YoloDetectionDataset(img_val, lbl_val, classes, transforms=tform, label_offset=CONFIG[\"frcnn\"][\"label_offset\"])\n",
    "        cat_offset = CONFIG[\"frcnn\"][\"cat_id_offset\"]\n",
    "        eval_conf = 0.0\n",
    "    elif model_type == \"retinanet\":\n",
    "        model = build_retinanet(num_fg)\n",
    "        ds_tr = YoloDetectionDataset(img_tr, lbl_tr, classes, transforms=tform, label_offset=CONFIG[\"retinanet\"][\"label_offset\"])\n",
    "        ds_val = YoloDetectionDataset(img_val, lbl_val, classes, transforms=tform, label_offset=CONFIG[\"retinanet\"][\"label_offset\"])\n",
    "        cat_offset = CONFIG[\"retinanet\"][\"cat_id_offset\"]\n",
    "        eval_conf = CONFIG[\"retinanet\"][\"eval_conf\"]\n",
    "    elif model_type == \"fcos\":\n",
    "        model = build_fcos(num_fg)\n",
    "        ds_tr = YoloDetectionDataset(img_tr, lbl_tr, classes, transforms=tform, label_offset=CONFIG[\"fcos\"][\"label_offset\"])\n",
    "        ds_val = YoloDetectionDataset(img_val, lbl_val, classes, transforms=tform, label_offset=CONFIG[\"fcos\"][\"label_offset\"])\n",
    "        cat_offset = CONFIG[\"fcos\"][\"cat_id_offset\"]\n",
    "        eval_conf = CONFIG[\"fcos\"][\"eval_conf\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model_type\")\n",
    "\n",
    "    dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=CONFIG[\"tv\"][\"batch_train\"], shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
    "    dl_val = torch.utils.data.DataLoader(ds_val, batch_size=CONFIG[\"tv\"][\"batch_val\"], shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "\n",
    "    model.to(device)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optim = torch.optim.AdamW(params, lr=lr, weight_decay=wd)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs)\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        ep_loss = 0.0\n",
    "        for imgs, tgts, _paths in tqdm(dl_tr, desc=f\"{model_type} epoch {ep + 1}/{epochs}\"):\n",
    "            imgs = [img.to(device) for img in imgs]\n",
    "            tgts = [{k: v.to(device) for k, v in t.items()} for t in tgts]\n",
    "            loss_dict = model(imgs, tgts)\n",
    "            loss = sum(loss_dict.values())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            ep_loss += loss.item()\n",
    "        sched.step()\n",
    "        print(f\"Epoch {ep + 1}: loss={ep_loss / max(1, len(dl_tr)):.4f}\")\n",
    "\n",
    "    out_dir = work_root / f\"torchvision_{model_type}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ckpt = out_dir / \"model_final.pth\"\n",
    "    torch.save(model.state_dict(), ckpt)\n",
    "    print(\"Saved\", ckpt)\n",
    "\n",
    "    dl_val_eval = torch.utils.data.DataLoader(ds_val, batch_size=CONFIG[\"tv\"][\"batch_val\"], shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "    coco_gt, dets = evaluate_to_coco_json(\n",
    "        model,\n",
    "        dl_val_eval,\n",
    "        device,\n",
    "        work_root / \"coco_val.json\",\n",
    "        score_thresh=eval_conf,\n",
    "        category_id_offset=cat_offset,\n",
    "        maxDets=CONFIG[\"coco\"][\"maxDets\"]\n",
    "    )\n",
    "    metrics = coco_eval_from_results(coco_gt, dets, maxDets=CONFIG[\"coco\"][\"maxDets\"])\n",
    "    return model, metrics, out_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c7c6c",
   "metadata": {},
   "source": [
    "\n",
    "## Inference visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_model_preds_yolo(yolo_model, img_paths: List[Path], out_dir: Path, title_prefix: str, device: str):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    res = yolo_model.predict([str(p) for p in img_paths], imgsz=CONFIG[\"yolo\"][\"imgsz\"], conf=0.25, device=0 if device.startswith(\"cuda\") else \"cpu\", verbose=False)\n",
    "    for p, r in zip(img_paths, res):\n",
    "        vis = r.plot()[:, :, ::-1]\n",
    "        out_path = out_dir / f\"{title_prefix}_{p.stem}.png\"\n",
    "        cv2.imwrite(str(out_path), cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def visualize_model_preds_torchvision(model, img_paths: List[Path], out_dir: Path, title_prefix: str, device: str, class_names: List[str]):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.eval()\n",
    "    tform = transforms.Compose([transforms.ToTensor()])\n",
    "    for p in img_paths:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        inp = tform(img).to(device)\n",
    "        out = model([inp])[0]\n",
    "        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "        boxes = out[\"boxes\"].detach().cpu().numpy()\n",
    "        scores = out[\"scores\"].detach().cpu().numpy()\n",
    "        labels = out[\"labels\"].detach().cpu().numpy()\n",
    "        for b, s, lb in zip(boxes, scores, labels):\n",
    "            if s < 0.25:\n",
    "                continue\n",
    "            x0, y0, x1, y1 = map(int, b)\n",
    "            cls_idx = lb if lb >= 1 else lb + 1\n",
    "            cls_name = class_names[cls_idx - 1] if 1 <= cls_idx <= len(class_names) else str(lb)\n",
    "            cv2.rectangle(img_cv, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "            cv2.putText(img_cv, f\"{cls_name} {s:.2f}\", (x0, max(0, y0 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        out_path = out_dir / f\"{title_prefix}_{p.stem}.png\"\n",
    "        cv2.imwrite(str(out_path), img_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def702c8",
   "metadata": {},
   "source": [
    "\n",
    "## End-to-end execution\n",
    "\n",
    "Run the cell below to execute the full pipeline. It will:\n",
    "\n",
    "1. Resolve the dataset location (download from Kaggle if requested).\n",
    "2. Prepare YOLO-formatted labels and COCO annotations.\n",
    "3. Train YOLOv8, Faster R-CNN, RetinaNet, and FCOS with the configured hyperparameters.\n",
    "4. Compute COCO metrics and store qualitative predictions under the `outputs/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219dd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure configuration reflects quick overrides\n",
    "CONFIG[\"yolo\"][\"epochs\"] = EPOCHS_YOLO\n",
    "CONFIG[\"tv\"][\"epochs\"] = EPOCHS_TV\n",
    "\n",
    "root = PROJECT_ROOT\n",
    "outputs = root / CONFIG[\"outputs_dir\"]\n",
    "outputs.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = device_info()\n",
    "\n",
    "# Resolve dataset location\n",
    "if DATASET_DIR is not None:\n",
    "    raw_ds = resolve_tomato_root(Path(DATASET_DIR))\n",
    "    print(\"Using dataset_dir:\", raw_ds)\n",
    "elif SKIP_KAGGLE:\n",
    "    raw_ds = resolve_tomato_root(root / \"datasets\")\n",
    "    print(\"Using local datasets (skip_kaggle):\", raw_ds)\n",
    "else:\n",
    "    try:\n",
    "        raw_ds = download_variant10_dataset(root)\n",
    "    except Exception as e:\n",
    "        print(\"Kaggle download failed, using ./datasets; ensure it contains images and annotations.\", e)\n",
    "        raw_ds = resolve_tomato_root(root / \"datasets\")\n",
    "\n",
    "sample_imgs = [p for p in raw_ds.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "print(\"Raw dataset root:\", raw_ds)\n",
    "print(\"Found images:\", len(sample_imgs))\n",
    "if len(sample_imgs) == 0:\n",
    "    raise RuntimeError(f\"No images found under {raw_ds}. Point DATASET_DIR to a folder that contains images and annotations.\")\n",
    "\n",
    "classes = CONFIG[\"classes\"]\n",
    "work = root / CONFIG[\"work_dir\"]\n",
    "data_yaml, work_root = prepare_yolo_and_coco(raw_ds, work, classes)\n",
    "\n",
    "# Sanity check renders\n",
    "sanity_dir = outputs / \"sanity\"\n",
    "sanity_dir.mkdir(parents=True, exist_ok=True)\n",
    "samples = list((work_root / \"images\" / \"train\").glob(\"*\"))[:6]\n",
    "for p in samples:\n",
    "    lbl = work_root / \"labels\" / \"train\" / f\"{p.stem}.txt\"\n",
    "    img = draw_yolo_boxes(p, lbl, classes)\n",
    "    cv2.imwrite(str(sanity_dir / f\"train_{p.stem}.png\"), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Train YOLOv8\n",
    "print(\"Training YOLOv8...\")\n",
    "yolo_model, yolo_metrics = train_eval_yolo(data_yaml, work_root, device)\n",
    "\n",
    "# Train TorchVision detectors\n",
    "print(\"Training Faster R-CNN...\")\n",
    "frcnn_model, frcnn_metrics, frcnn_dir = train_torchvision_detector(\n",
    "    \"fasterrcnn\", work_root, device, classes,\n",
    "    epochs=CONFIG[\"tv\"][\"epochs\"], lr=CONFIG[\"tv\"][\"lr\"], wd=CONFIG[\"tv\"][\"weight_decay\"]\n",
    ")\n",
    "\n",
    "print(\"Training RetinaNet...\")\n",
    "retina_model, retina_metrics, retina_dir = train_torchvision_detector(\n",
    "    \"retinanet\", work_root, device, classes,\n",
    "    epochs=CONFIG[\"tv\"][\"epochs\"], lr=CONFIG[\"tv\"][\"lr\"], wd=CONFIG[\"tv\"][\"weight_decay\"]\n",
    ")\n",
    "\n",
    "print(\"Training FCOS...\")\n",
    "fcos_model, fcos_metrics, fcos_dir = train_torchvision_detector(\n",
    "    \"fcos\", work_root, device, classes,\n",
    "    epochs=CONFIG[\"tv\"][\"epochs\"], lr=CONFIG[\"tv\"][\"lr\"], wd=CONFIG[\"tv\"][\"weight_decay\"]\n",
    ")\n",
    "\n",
    "rows = [\n",
    "    {\"model\": \"YOLOv8n\", \"mAP_50_95\": yolo_metrics[\"mAP_50_95\"], \"mAP_50\": yolo_metrics[\"mAP_50\"], \"AR100\": yolo_metrics[\"AR100\"], \"AP_tomato\": yolo_metrics[\"AP_per_class\"].get(\"tomato\", float('nan'))},\n",
    "    {\"model\": \"Faster R-CNN\", \"mAP_50_95\": frcnn_metrics[\"mAP_50_95\"], \"mAP_50\": frcnn_metrics[\"mAP_50\"], \"AR100\": frcnn_metrics[\"AR100\"], \"AP_tomato\": frcnn_metrics[\"AP_per_class\"].get(\"tomato\", float('nan'))},\n",
    "    {\"model\": \"RetinaNet\", \"mAP_50_95\": retina_metrics[\"mAP_50_95\"], \"mAP_50\": retina_metrics[\"mAP_50\"], \"AR100\": retina_metrics[\"AR100\"], \"AP_tomato\": retina_metrics[\"AP_per_class\"].get(\"tomato\", float('nan'))},\n",
    "    {\"model\": \"FCOS\", \"mAP_50_95\": fcos_metrics[\"mAP_50_95\"], \"mAP_50\": fcos_metrics[\"mAP_50\"], \"AR100\": fcos_metrics[\"AR100\"], \"AP_tomato\": fcos_metrics[\"AP_per_class\"].get(\"tomato\", float('nan'))}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "metrics_csv = outputs / \"metrics_comparison.csv\"\n",
    "df.to_csv(metrics_csv, index=False)\n",
    "print(\"Wrote\", metrics_csv)\n",
    "df\n",
    "\n",
    "# Visualizations\n",
    "vis_dir = outputs / \"vis\"\n",
    "vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "val_imgs = list((work_root / \"images\" / \"val\").glob(\"*\"))[:6]\n",
    "visualize_model_preds_yolo(yolo_model, val_imgs, vis_dir, \"yolo\", device)\n",
    "visualize_model_preds_torchvision(frcnn_model, val_imgs, vis_dir, \"fasterrcnn\", device, classes)\n",
    "visualize_model_preds_torchvision(retina_model, val_imgs, vis_dir, \"retinanet\", device, classes)\n",
    "visualize_model_preds_torchvision(fcos_model, val_imgs, vis_dir, \"fcos\", device, classes)\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
